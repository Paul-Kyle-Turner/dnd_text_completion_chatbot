{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95fe1cc-5686-44dc-9acc-bf2a850a9b14",
   "metadata": {},
   "source": [
    "# TTRPG Chatbot\n",
    "\n",
    "Really this bot can easily be made into anything by changing the prompt slightly.  It works with pinecone to keep track of the conversation as it happens and retrieve the most useful bits of the conversation for the bot.  Note that I am saving a good amount (including the vector) to the json logs saved in the logs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcacccc6-0f3a-4338-8a34-4b0818426e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pinecone\n",
    "import openai\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "embedding_dimensions = 1536\n",
    "model_engine = \"text-embedding-ada-002\"\n",
    "pinecone_name = \"dnd-rules-lawyer\"\n",
    "pinecone_region = \"asia-southeast1-gcp\" # Pinecone calls this the environement ? strange\n",
    "\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_region)\n",
    "index = pinecone.Index(pinecone_name)\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b8871-b049-4ede-bbde-a108fc64c221",
   "metadata": {},
   "source": [
    "Below are the helper functions for using the chatbot.\n",
    "\n",
    "Beware the commented code.  It will delete your vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6127f536-9113-442f-b7df-ed068e01529f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results = index.query(vector=[0 for i in range(1536)], top_k=1000)\n",
    "#print(results)\n",
    "#index.delete([x[\"id\"] for x in results[\"matches\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277bf22-0252-45f4-944d-5554483383cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_metadata(content, user, vector):\n",
    "    create_time = time.time()\n",
    "    return {\n",
    "        \"Timestamp\": create_time,\n",
    "        \"User\": \"User\",\n",
    "        \"Message\": content,\n",
    "        \"Vector\": vector\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4141377-ad3d-4439-8257-f639014fb763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_message_hash(content):\n",
    "    return str(hash(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19c3e6-3dfd-452b-98cd-8a03aba8968e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_openai_embeddings(content, engine=\"text-embedding-ada-002\"):\n",
    "    content = content.encode(encoding=\"ASCII\", errors=\"ignore\").decode()  # fix unicode errors\n",
    "    response = openai.Embedding.create(input=content, engine=engine)\n",
    "    vector = response['data'][0]['embedding']\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa721f93-1fc5-4aff-9fc7-d48f03abe7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def openai_completion(prompt, model=\"text-davinci-003\", temperature=0, top_p=1.0, max_tokens=400, freq_pen=0.0, pres_pen=0.0, max_retry=5):\n",
    "        retry = 0\n",
    "        prompt = prompt.encode(encoding=\"ASCII\", errors=\"ignore\").decode()\n",
    "        while True:\n",
    "            try:\n",
    "                response = openai.Completion.create(\n",
    "                    model=model,\n",
    "                    prompt=prompt,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    top_p=top_p,\n",
    "                    frequency_penalty=freq_pen,\n",
    "                    presence_penalty=pres_pen\n",
    "                )\n",
    "                text = response[\"choices\"][0][\"text\"].strip()\n",
    "                text = re.sub(r'[\\r\\n]+', \"\\n\", text)\n",
    "                text = re.sub(r'[\\t]+', \" \", text)\n",
    "                return text\n",
    "                    \n",
    "            except Exception as e:\n",
    "                retry += 1\n",
    "                if retry >= max_retry:\n",
    "                    return f\"GPT3 error: {e}\"\n",
    "                print(\"Error in communication with openai.\")\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946ba28-045c-4f71-8804-115b9da6970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as open_file:\n",
    "        return json.load(open_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e7ca1-83e8-4abd-abe4-3cbf3c65b79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_conversation(results):\n",
    "    result = list()\n",
    "    for m in results[\"matches\"]:\n",
    "        if m['id'] == \"GPT3 error: Unrecognized request argument supplied: stops\": # For some reason pinecone will not remove this from the database\n",
    "            continue\n",
    "        info = load_json(f'./logs/{m[\"id\"]}.json')\n",
    "        result.append(info)\n",
    "    ordered = sorted(result, key=lambda d: d['Timestamp'], reverse=False)\n",
    "    messages = [i[\"Message\"] for i in ordered]\n",
    "    return \"\\n\".join(messages).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b13f5-e602-4e5e-b82b-2d8d80f1b97f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(file_name):\n",
    "    with open(file_name) as open_file:\n",
    "        return open_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2111b79-af9c-43ba-b9bf-2f14e77a8cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_json(filepath, content):\n",
    "    with open(filepath + \".json\", \"w+\") as f:\n",
    "        json.dump(content, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806696b8-fee5-4f95-ad7c-e81d7006bf3c",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "Here is the interesting part of the bot.  This prompt below can be changed to fit your needs and used to create chatbots that have l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7550b5-e508-4035-98d7-95918d162458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_prompt(\"prompt.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6409f-be38-47ac-9611-36a234b62964",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594352d5-f9f4-47ea-ac97-e28a758417cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = \"USER\"\n",
    "top_k = 15\n",
    "prompt_file = \"prompt.txt\"\n",
    "logs_path = \"./logs/\"\n",
    "\n",
    "print(\"Welcome to the TTRPG chatbot.  Ask a ruling here!\")\n",
    "while True:\n",
    "    payload = list()\n",
    "    \n",
    "    # Message Meta\n",
    "    message = input(\"\\n\\n USER: \")\n",
    "    message_hash = create_message_hash(message)\n",
    "\n",
    "    # Create embedding of new message\n",
    "    message_vector = get_openai_embeddings(message)\n",
    "    \n",
    "    # Save metadata about vector\n",
    "    metadata = create_metadata(message, user, message_vector)\n",
    "    save_json(logs_path + str(message_hash), metadata)\n",
    "    \n",
    "    # Append to payload for later indexing\n",
    "    # Send to Pinecone after gpt message\n",
    "    payload.append((message_hash, message_vector))\n",
    "    \n",
    "    # Query for relevant messages, generate response\n",
    "    results = index.query(vector=message_vector, top_k=top_k)\n",
    "    conversation = load_conversation(results)\n",
    "    prompt = get_prompt(prompt_file).replace(\"<PREVIOUS_CONVERSATION>\", conversation).replace(\"<MESSAGE>\", message)\n",
    "    \n",
    "    # Generate the response from the large lang model\n",
    "    output = openai_completion(prompt)\n",
    "    output_hash = create_message_hash(output)\n",
    "    \n",
    "    # Embed the output\n",
    "    output_vector = get_openai_embeddings(output)\n",
    "\n",
    "    # Save the output metadata\n",
    "    metadata = create_metadata(output, \"The Rule Lich\", output_vector)\n",
    "    save_json(logs_path + str(output_hash), metadata)\n",
    "    \n",
    "    # Append to the payload the response from gpt\n",
    "    payload.append((output_hash, output_vector))\n",
    "    \n",
    "    # Upsert to the pinecone database\n",
    "    index.upsert(payload)\n",
    "    \n",
    "    # Print responce to the message\n",
    "    print(f'\\n {output}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone-chroma",
   "language": "python",
   "name": "pinecone-chroma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
